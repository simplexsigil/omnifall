<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriately as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description"
    content="OmniFall is a unified benchmark for video-based human fall detection, combining eight staged datasets and real-world fall videos to enable fair cross-dataset evaluation and measurement of generalization capabilities.">

  <meta property="og:title" content="OmniFall: A Unified Staged-to-Wild Benchmark for Human Fall Detection" />
  <meta property="og:description"
    content="OmniFall unifies eight public fall detection datasets under a consistent ten-class taxonomy and introduces OOPS-Fall for evaluation of fall detection systems in real-world scenarios." />
  <meta property="og:url" content="https://davidschneider.ai/omnifall" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="https://davidschneider.ai/omnifall/static/images/banner.webp" />
  <meta property="og:image:width" content="1483" />
  <meta property="og:image:height" content="678" />

  <meta name="twitter:title" content="OmniFall: A Unified Staged-to-Wild Benchmark for Human Fall Detection">
  <meta name="twitter:description"
    content="OmniFall unifies eight public fall detection datasets under a consistent taxonomy and introduces OOPS-Fall for real-world evaluation, revealing significant performance gaps between staged and in-the-wild settings.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
  <meta name="twitter:image" content="https://davidschneider.ai/omnifall/static/images/banner.webp">
  <meta name="twitter:card" content="summary_large_image">

  <!-- Keywords for your paper to be indexed by -->
  <meta name="keywords"
    content="Fall detection, benchmark dataset, computer vision, human activity recognition, healthcare monitoring, elderly care, OOPS-Fall, staged-to-wild, unified taxonomy, cross-dataset evaluation">

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>OmniFall: A Unified Staged-to-Wild Benchmark for Human Fall Detection</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link href="static/css/prism.css" rel="stylesheet" />
  <script src="static/js/prism.js"></script>
  <script src="static/js/prism-bibtex.min.js"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img
  src="static/images/omnifall_logo.webp"
  alt="O"
  class="logo-o">mniFall: A Unified Staged-to-Wild Benchmark for Human Fall Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://davidschneider.ai" target="_blank">David Schneider</a></sup><sup>†</sup>,
              </span>
              <span class="author-block">
                <a href="https://cvhci.iar.kit.edu/people_2240.php" target="_blank">Zdravko Marinov</a></sup>,
              </span>
              <span class="author-block">
                Rafael Baur</sup>,
              </span>
              <span class="author-block">
                <a href="https://zeyun-zhong.github.io/" target="_blank">Zeyun Zhong</a></sup>,
              </span>
              <span class="author-block">
                Rodi Düger</sup>,
              </span>
              <span class="author-block">
                <a href="https://cvhci.iar.kit.edu/people_596.php" target="_blank">Rainer Stiefelhagen</a></sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Karlsruhe Institute of Technology</span> <br>
              <span class="eql-cntrb"><small>
                  <sup>†</sup>Corresponding author: <a
                    href="mailto:david.schneider@kit.edu">david.schneider∂kit.edu</a></small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="http://arxiv.org/abs/2505.19889" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/datasets/simplexsigil2/omnifall" target="_blank"
                    class="external-link button is-normal is-rounded is-dark is-primary has-background-primary-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!--
                <span class="link-block">
                  <a href="https://s.kit.edu/omnifall" target="_blank"
                    class="external-link button is-normal is-rounded is-dark is-primary has-background-primary-dark">
                    <span class="icon">
                      <i class="fa fa-database"></i>
                    </span>
                    <span>Features</span>
                  </a>
                </span>
                -->
                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/simplexsigil/omnifall-experiments" target="_blank"
                    class="external-link button is-normal is-rounded is-dark is-info has-background-info-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://cvhci.iar.kit.edu/~dschneider/published/omnifall_examples/grid.html" target="_blank"
                    class="external-link button is-normal is-rounded is-dark is-info has-background-info-dark">
                    <span class="icon">
                      <i class="fas fa-video"></i>
                    </span>
                    <span>Examples</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="hero-body" style="display: flex; justify-content: space-between;">
          <video poster="" id="tree" autoplay controls muted loop height="100%" width="60%">
            <source src="static/videos/Subject1Activity4Trial2Camera2_blurred.mp4" type="video/mp4">
          </video>
          <video poster="" id="anotherVideo" autoplay controls muted loop height="100%" width="30%">
            <source src="static/videos/BackinUSSRFebruary2019_FailArmy46.mp4" type="video/mp4">
          </video>
        </div>
        <h2 class="subtitle has-text-centered">
          Examples of fall detection scenarios. <br></bre><strong>Left:</strong> Staged fall from UP-Fall dataset. 
          <strong>Right</strong>: Real-world fall from OOPS-Fall dataset.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->


  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Falls remain a leading cause of fatal and non-fatal injury in older adults, claiming about 684,000 lives globally each year. Early detection of falls and especially of sustained fallen states is critical for timely medical intervention. However, current video-based fall detection research mostly relies on small, staged datasets with significant domain biases concerning background, lighting, and camera setup, resulting in unknown real-world performance. These limitations have hindered progress in developing systems that can reliably operate in diverse environments where they are most needed.
            </p>
            <p>
              We introduce <strong>OmniFall</strong>, a comprehensive benchmark that unifies eight public fall detection datasets (~14 h of recordings, ~42 h of multiview data, 101 subjects, 29 camera views) under a consistent ten‑class taxonomy with standardized evaluation protocols. By distinguishing between transient actions (fall, sit down, lie down, stand up) and static states (fallen, sitting, lying, standing, walking, other), our benchmark provides complete video segmentation labels and enables fair cross-dataset comparison previously impossible with incompatible annotation schemes.
            </p>
            <p>
              For real-world evaluation, we curate <strong>OOPS‑Fall</strong> from genuine accident videos and establish a staged-to-wild protocol measuring generalization from controlled to uncontrolled environments. Our experiments with frozen pre-trained backbones such as I3D or VideoMAE reveal significant performance gaps between in-distribution and in-the-wild scenarios, highlighting critical challenges in developing robust fall detection systems. These findings underscore the importance of evaluating fall detection approaches on genuine falls and demonstrate the value of OmniFall as a comprehensive benchmark for advancing the state of the art in this critical healthcare domain.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Dataset overview -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <h2 class="title is-3">Dataset Overview</h2>
          <div class="content has-text-justified">
            <p>
              The OmniFall benchmark represents a significant advance in fall detection research by unifying eight diverse public datasets under a common framework. Previous research has been hindered by the fragmentation of data across incompatible annotation schemes and evaluation protocols, making cross-dataset comparisons nearly impossible. Our unified approach addresses this fundamental limitation through comprehensive re-annotation and standardization.
            </p>
            <div class="columns is-centered">
              <div class="column is-10">
                <figure class="has-text-centered">
                  <img src="static/images/dataset_examples.webp" alt="Example videos from the OmniFall benchmark" style="width: 100%;" />
                  <figcaption>Figure 1: Example videos from the OmniFall benchmark showcasing the diversity of camera views, subjects, and environments.</figcaption>
                </figure>
              </div>
            </div>
            <p>
              At the core of OmniFall is a consistent 10-class taxonomy that distinguishes between transient actions and static states. The transient actions include fall, sit down, lie down, and stand up—capturing the dynamic transitions in human movement. The static states encompass fallen, sitting, lying, standing, walking, and other—representing persistent postures particularly relevant for detecting prolonged "long-lie" episodes that are medically critical in eldercare scenarios. This nuanced classification enables detection systems to identify not just the momentary fall event but the sustained fallen state that often leads to adverse medical outcomes.
            </p>
            <div class="columns is-centered">
              <div class="column is-10">
                <figure class="has-text-centered">
                  <img src="static/images/timeline_visualization.webp" alt="Example videos from the OmniFall benchmark" style="width: 100%;" />
                  <figcaption>Figure 2: Our cross-dataset compatible annotations for CMDFall and actoin segment predictions of MS-TCN++ trained on OmniFall.</figcaption>
                </figure>
              </div>
            </div>
            <p>
              The consolidated benchmark comprises approximately 14 hours of unique recordings that expand to 42 hours when accounting for multiview data. The combination of datasets leads to diversity: 101 subjects performing activities across 29 different camera views, creating a rich tapestry of scenarios that challenge detection systems to generalize across variations in human appearance, environmental conditions, and camera perspectives.
            </p>

            <p>
              The Staged Fall Superset integrates diverse datasets including CMDFall, UP-Fall, Le2i, GMDCSA, EDF and OCCU, MCFD, and CAUCAFall. Each dataset brings unique strengths to the benchmark, collectively creating a comprehensive resource for training and evaluating fall detection systems.
            </p>
            <div class="columns is-centered">
              <div class="column is-10">
                <figure class="has-text-centered">
                  <img src="static/images/shares_and_durations.webp" alt="Example videos from the OmniFall benchmark" style="width: 100%;" />
                  <figcaption>Figure 3: Segmented share of each label within datasets and total single view duration.</figcaption>
                </figure>
              </div>
            </div>
            <p>
              Complementing these staged datasets, we introduce OOPS-Fall—a curated collection of genuine accident videos from uncontrolled environments. This addition enables our pioneering staged-to-wild evaluation protocol, which measures how well systems generalize from controlled laboratory settings to the unpredictable complexities of real-world scenarios where fall detection is most needed.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End dataset overview -->

  <!-- Key contributions -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <h2 class="title is-3">Key Contributions</h2>
          <div class="content has-text-justified">
            <p>
              OmniFall addresses the fragmentation problem in fall detection research by unifying eight public datasets (totaling ~42 hours of multiview data) under a standardized 10-class taxonomy. Previous research has relied on isolated datasets with incompatible annotations, making cross-dataset comparisons difficult. Our re-annotation work enables direct evaluation across datasets with 101 subjects across 29 camera views, providing a diverse benchmark for fall detection research.
            </p>
            <p>
              We introduce OOPS-Fall, a curated collection of genuine fall accidents from real-world environments. While existing fall detection research has primarily used staged scenarios, OOPS-Fall contains authentic human reactions, diverse environments, and natural camera movements that reflect real emergency situations. This resource enables evaluation in conditions that more closely match the environments where fall detection systems would ultimately be deployed.
            </p>
            <p>
              The staged-to-wild evaluation protocol quantifies the gap between laboratory performance and real-world effectiveness. Our measurements show significant performance degradation when systems encounter genuine falls compared to staged scenarios, providing an assessment metric for potential deployment viability. This protocol helps address the evaluation challenges that have been a barrier to wider adoption of video-based fall detection systems.
            </p>
            <p>
              Benchmarking of various video architectures reveals insights into domain shift challenges in fall detection. The experiments show that transformer-based models with high accuracy on in-distribution data can underperform on real-world falls compared to architectures with more modest laboratory results. These findings suggest important considerations for developing fall detection systems intended for deployment in healthcare environments.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End key contributions -->

  <!-- Findings -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <h2 class="title is-3">Key Findings</h2>
          <div class="content has-text-justified">
            <p>
              We observe substantial performance degradation when models transition from staged datasets to real-world scenarios. State-of-the-art models that excel on in-distribution test sets show alarming accuracy drops when confronted with genuine falls from OOPS-Fall, revealing a critical domain gap that hinders practical deployment in healthcare settings where reliability is essential.
            </p>
            <p>
              The Inflated 3D ConvNet (I3D) architecture demonstrates surprising robustness to domain shifts despite its modest in-distribution performance. This counterintuitive finding suggests that architectural choices optimizing for controlled environments may not translate to real-world generalization. I3D's ability to maintain performance across domain boundaries indicates it captures more domain-invariant aspects of falls, likely due to its multi-scale processing approach.
            </p>
            <p>
              Transformer-based models like VideoMAE excel on controlled datasets but struggle with domain generalization. Despite pre-training on diverse datasets like Kinetics-400, these models overfit to the visual characteristics of staged falls, failing to extract generalizable motion patterns. This challenges the assumption that larger, more powerful models automatically provide better generalization and highlights the importance of architectural inductive biases for cross-domain robustness.
            </p>
            <p>
              Fall detection systems generalize better across different subjects than across different camera viewpoints, indicating that viewpoint variation presents a more challenging dimension than subject appearance variation. The significant performance drop in cross-view evaluation emphasizes the importance of training with diverse camera perspectives for robust deployment in multi-camera monitoring scenarios typical in care facilities.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End findings -->

  <!-- Datasets overview -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column">
          <h2 class="title is-3">Datasets</h2>
          <div class="content has-text-justified">
            <p>
              The OmniFall benchmark unifies the following datasets, each accessed under appropriate permissions or licenses:
            </p>
            
            <div class="table-container">
              <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
                <thead>
                  <tr>
                    <th>Dataset</th>
                    <th>Duration</th>
                    <th>Subjects</th>
                    <th>Views/Rooms</th>
                    <th>License</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><strong><a href="https://www.mica.edu.vn/perso/Tran-Thi-Thanh-Hai/CMDFALL.html" target="_blank" class="has-text-link">CMDFall</a></strong></td>
                    <td>7h 25m</td>
                    <td>50</td>
                    <td>7 synchronized views</td>
                    <td>Permission from authors</td>
                  </tr>
                  <tr>
                    <td><strong><a href="https://sites.google.com/up.edu.mx/har-up/" target="_blank" class="has-text-link">UP Fall</a></strong></td>
                    <td>4h 35m</td>
                    <td>17</td>
                    <td>2 synchronized views</td>
                    <td>Permission from authors</td>
                  </tr>
                  <tr>
                    <td><strong><a href="https://search-data.ubfc.fr/imvia/FR-13002091000019-2024-04-09_Fall-Detection-Dataset.html" target="_blank" class="has-text-link">Le2i</a></strong></td>
                    <td>47m</td>
                    <td>9</td>
                    <td>6 different rooms</td>
                    <td>CC-BY-NC-SA 3.0</td>
                  </tr>
                  <tr>
                    <td><strong><a href="https://github.com/ekramalam/GMDCSA24-A-Dataset-for-Human-Fall-Detection-in-Videos" target="_blank" class="has-text-link">GMDCSA24</a></strong></td>
                    <td>21m</td>
                    <td>4</td>
                    <td>3 rooms</td>
                    <td>MIT</td>
                  </tr>
                  <tr>
                    <td><strong><a href="https://data.mendeley.com/datasets/7w7fccy7ky/4" target="_blank" class="has-text-link">CAUCAFall</a></strong></td>
                    <td>16m</td>
                    <td>10</td>
                    <td>1 room</td>
                    <td>CC BY 4.0</td>
                  </tr>
                  <tr>
                    <td><strong><a href="https://doi.org/10.5281/zenodo.15494102" target="_blank" class="has-text-link">EDF</a></strong></td>
                    <td>13m</td>
                    <td>5</td>
                    <td>2 views synchronized</td>
                    <td>Permission from authors</td>
                  </tr>
                  <tr>
                    <td><strong><a href="https://doi.org/10.5281/zenodo.15494102" target="_blank" class="has-text-link">OCCU</a></strong></td>
                    <td>14m</td>
                    <td>5</td>
                    <td>2 views not synchronized</td>
                    <td>Permission from authors</td>
                  </tr>
                  <tr>
                    <td><strong><a href="https://www.iro.umontreal.ca/~labimage/Dataset/" target="_blank" class="has-text-link">MCFD</a></strong></td>
                    <td>12m</td>
                    <td>1</td>
                    <td>8 views</td>
                    <td>Permission from authors</td>
                  </tr>
                  <tr>
                    <td><strong><a href="https://oops.cs.columbia.edu/data/" target="_blank" class="has-text-link">OOPS-Fall</a></strong></td>
                    <td>Varies</td>
                    <td>Varied</td>
                    <td>Varied</td>
                    <td>CC BY-NC-SA 4.0</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <p>
              The combined Staged Fall Superset encompasses approximately 14 hours of unique recordings, expanding to 42 hours when accounting for multiview data. This diversity of 101 subjects performing activities across 29 different camera views creates a rich tapestry of scenarios that challenge detection systems to generalize across variations in human appearance, environmental conditions, and camera perspectives.
            </p>
            <p>
              For real-world testing, OOPS-Fall provides a curated collection of genuine accident videos from uncontrolled environments, allowing evaluation of how well systems generalize from controlled laboratory settings to unpredictable real-world scenarios.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End datasets overview -->

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre class="language-bib"><code class="language-bib">@inproceedings{schneider2025omnifall,
  title={OmniFall: A Unified Staged-to-Wild Benchmark for Human Fall Detection},
  author={Schneider, David and Marinov, Zdravko and Baur, Rafael and Zhong, Zeyun and D{\"u}ger, Rodi and Stiefelhagen, Rainer},
  booktitle={ },
  year={2025}
},

@inproceedings{omnifall_cmdfall,
  title={A multi-modal multi-view dataset for human fall analysis and preliminary investigation on modality},
  author={Tran, Thanh-Hai and Le, Thi-Lan and Pham, Dinh-Tan and Hoang, Van-Nam and Khong, Van-Minh and Tran, Quoc-Toan and Nguyen, Thai-Son and Pham, Cuong},
  booktitle={2018 24th International Conference on Pattern Recognition (ICPR)},
  pages={1947--1952},
  year={2018},
  organization={IEEE}
},

@article{omnifall_up-fall,
  title={UP-fall detection dataset: A multimodal approach},
  author={Mart{\'\i}nez-Villase{\~n}or, Lourdes and Ponce, Hiram and Brieva, Jorge and Moya-Albor, Ernesto and N{\'u}{\~n}ez-Mart{\'\i}nez, Jos{\'e} and Pe{\~n}afort-Asturiano, Carlos},
  journal={Sensors},
  volume={19},
  number={9},
  pages={1988},
  year={2019},
  publisher={MDPI}
},

@article{omnifall_le2i,
  title={Optimized spatio-temporal descriptors for real-time fall detection: comparison of support vector machine and Adaboost-based classification},
  author={Charfi, Imen and Miteran, Johel and Dubois, Julien and Atri, Mohamed and Tourki, Rached},
  journal={Journal of Electronic Imaging},
  volume={22},
  number={4},
  pages={041106--041106},
  year={2013},
  publisher={Society of Photo-Optical Instrumentation Engineers}
},

@article{omnifall_gmdcsa,
  title={GMDCSA-24: A dataset for human fall detection in videos},
  author={Alam, Ekram and Sufian, Abu and Dutta, Paramartha and Leo, Marco and Hameed, Ibrahim A},
  journal={Data in Brief},
  volume={57},
  pages={110892},
  year={2024},
  publisher={Elsevier}
},

@article{omnifall_cauca,
  title={Dataset CAUCAFall},
  author={Eraso, Jose Camilo and Mu{\~n}oz, Elena and Mu{\~n}oz, Mariela and Pinto, Jesus},
  journal={Mendeley Data},
  volume={4},
  year={2022}
},

@inproceedings{omnifall_edf_occu,
  title={Evaluating depth-based computer vision methods for fall detection under occlusions},
  author={Zhang, Zhong and Conly, Christopher and Athitsos, Vassilis},
  booktitle={International symposium on visual computing},
  pages={196--207},
  year={2014},
  organization={Springer}
},

@article{omnifall_mcfd,
  title={Multiple cameras fall dataset},
  author={Auvinet, Edouard and Rougier, Caroline and Meunier, Jean and St-Arnaud, Alain and Rousseau, Jacqueline},
  journal={DIRO-Universit{\'e} de Montr{\'e}al, Tech. Rep},
  volume={1350},
  pages={24},
  year={2010}
},

@inproceedings{omnifall_oops,
  title={Oops! predicting unintentional action in video},
  author={Epstein, Dave and Chen, Boyuan and Vondrick, Carl},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={919--929},
  year={2020}
}
</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <!-- Acknowledgements -->
  <section class="section" id="Acknowledgements">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgements</h2>
      This work has been supported by the Carl Zeiss Foundation through the
      JuBot project as well as by funding from the pilot program Core-Informatics of the Helmholtz Association (HGF).
      The authors acknowledge support by the state of Baden-Württemberg through bwHPC.
      Experiments were performed on the HoreKa supercomputer funded by the
      Ministry of Science, Research and the Arts Baden-Württemberg and by
      the Federal Ministry of Education and Research.
    </div>
  </section>
  <!-- End Acknowledgements -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              <small>
                This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank">Academic Project Page Template</a>. <br />
                <a href="https://davidschneider.ai/impressum/">Impressum</a>.
              </small>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</body>

</html>
